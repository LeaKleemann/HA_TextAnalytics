{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib2 as pathlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.utils import tokenize\n",
    "\n",
    "from langdetect import detect, DetectorFactory\n",
    "\n",
    "import joblib\n",
    "from optuna import Trial, create_study\n",
    "\n",
    "\n",
    "random_state=313\n",
    "DetectorFactory.seed = random_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath=pathlib.Path.cwd().joinpath('lens_org_data_files')\n",
    "datafiles=[x for x in datapath.iterdir()]\n",
    "models_path=pathlib.Path.cwd().joinpath('models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    clean = re.sub('<jats:[^>]*>', '', text)  # Entfernt alle jats Tags\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lekle\\AppData\\Local\\Temp\\ipykernel_23304\\856187700.py:3: DtypeWarning: Columns (22) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df=pd.read_csv(file)\n",
      "C:\\Users\\lekle\\AppData\\Local\\Temp\\ipykernel_23304\\856187700.py:3: DtypeWarning: Columns (22) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df=pd.read_csv(file)\n",
      "C:\\Users\\lekle\\AppData\\Local\\Temp\\ipykernel_23304\\856187700.py:3: DtypeWarning: Columns (24) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df=pd.read_csv(file)\n",
      "C:\\Users\\lekle\\AppData\\Local\\Temp\\ipykernel_23304\\856187700.py:3: DtypeWarning: Columns (24) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df=pd.read_csv(file)\n",
      "C:\\Users\\lekle\\AppData\\Local\\Temp\\ipykernel_23304\\856187700.py:3: DtypeWarning: Columns (22) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df=pd.read_csv(file)\n"
     ]
    }
   ],
   "source": [
    "dfs=[]\n",
    "for file in datafiles:\n",
    "    df=pd.read_csv(file)\n",
    "    dfs.append(df)\n",
    "\n",
    "data=pd.concat(dfs,ignore_index=True)\n",
    "data=data.dropna(subset=['Title', 'Abstract'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Lens ID', 'Title', 'Date Published', 'Publication Year',\n",
       "       'Publication Type', 'Source Title', 'ISSNs', 'Publisher',\n",
       "       'Source Country', 'Author/s', 'Abstract', 'Volume', 'Issue Number',\n",
       "       'Start Page', 'End Page', 'Fields of Study', 'Keywords', 'MeSH Terms',\n",
       "       'Chemicals', 'Funding', 'Source URLs', 'External URL', 'PMID', 'DOI',\n",
       "       'Microsoft Academic ID', 'PMCID', 'Citing Patents Count', 'References',\n",
       "       'Citing Works Count', 'Is Open Access', 'Open Access License',\n",
       "       'Open Access Colour'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(data.Title.isna().sum())\n",
    "print(data.Abstract.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<jats:p>Day-ahead solar irradiance forecasting is carried out using data from a tropical environment, Singapore. The performance of the weather research and forecasting (WRF) model is evaluated. We explore various combinations of physics configuration setups in the WRF model and propose a setup for the tropical regions. The WRF model is benchmarked using persistence and two seasonal time series models, namely, the exponential smoothing (ETS) and seasonal autoregressive integrated moving average (SARIMA) models. It is shown that the WRF model outperforms the SARIMA model and achieves accuracies comparable with persistence and ETS models. Persistence, ETS, and WRF models have relative root mean square errors (rRMSE) of about 55–57%. Furthermore, we find that by combining the forecasting outputs of WRF and ETS models, errors can be reduced to 49%.</jats:p>'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Abstract[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages=[]\n",
    "for x in data.Title:\n",
    "    try:\n",
    "        l=detect(x)\n",
    "    except:\n",
    "        l=False\n",
    "    languages.append(l)\n",
    "data['title_languages']=languages\n",
    "\n",
    "languages=[]\n",
    "for x in data.Abstract:\n",
    "    try:\n",
    "        l=detect(x)\n",
    "    except:\n",
    "        l=False\n",
    "    languages.append(l)\n",
    "data['abstract_languages']=languages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.title_languages.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "261492"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=data[(data['title_languages']=='en') & (data['abstract_languages']=='en')]\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     Day-Ahead Solar Irradiance Forecasting in a Tr...\n",
       "2     Assessment of a falling solid particle receive...\n",
       "4     Boron-Doped Silicon Diatom Frustules as a Phot...\n",
       "5     Renewable energy management through microgrid ...\n",
       "11    Thermal and electrical analysis of a linear pa...\n",
       "Name: Title, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Title.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261492\n",
      "81\n",
      "8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Source Country\n",
       "United Kingdom    44716\n",
       "United States     35457\n",
       "Germany           18078\n",
       "Switzerland       12517\n",
       "Netherlands       11864\n",
       "China              1796\n",
       "Egypt              1337\n",
       "India              1116\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(data))\n",
    "print(len(data['Source Country'].unique()))\n",
    "# data_sourceCountry_valueCounts=data['Source Country'].value_counts()\n",
    "data_sourceCountry_valueCounts=data[data.groupby('Source Country')['Source Country'].transform('count')>1000]\n",
    "print(len(data_sourceCountry_valueCounts['Source Country'].value_counts()))\n",
    "data_sourceCountry_valueCounts['Source Country'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Abstract']=data.Abstract.apply(clean_text)\n",
    "data.to_csv(\"cleaned_data/cleaned_data.csv\",sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     Day-ahead solar irradiance forecasting is carr...\n",
       "2     An advanced computational fluid dynamics (CFD)...\n",
       "4     An effective solar-powered silicon device for ...\n",
       "5     Abstract In this study, an isolated microgrid ...\n",
       "11    Thermal and electrical analysis of a linear pa...\n",
       "Name: Abstract, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Abstract.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lekle\\AppData\\Local\\Temp\\ipykernel_23304\\719415488.py:1: DtypeWarning: Columns (23,25) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data=pd.read_csv(\"cleaned_data/cleaned_data.csv\",sep=';')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Lens ID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Date Published</th>\n",
       "      <th>Publication Year</th>\n",
       "      <th>Publication Type</th>\n",
       "      <th>Source Title</th>\n",
       "      <th>ISSNs</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Source Country</th>\n",
       "      <th>...</th>\n",
       "      <th>Microsoft Academic ID</th>\n",
       "      <th>PMCID</th>\n",
       "      <th>Citing Patents Count</th>\n",
       "      <th>References</th>\n",
       "      <th>Citing Works Count</th>\n",
       "      <th>Is Open Access</th>\n",
       "      <th>Open Access License</th>\n",
       "      <th>Open Access Colour</th>\n",
       "      <th>title_languages</th>\n",
       "      <th>abstract_languages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>000-008-268-360-004</td>\n",
       "      <td>Day-Ahead Solar Irradiance Forecasting in a Tr...</td>\n",
       "      <td>2015-07-27</td>\n",
       "      <td>2015</td>\n",
       "      <td>journal article</td>\n",
       "      <td>Journal of Solar Energy Engineering</td>\n",
       "      <td>01996231; 15288986</td>\n",
       "      <td>ASME International</td>\n",
       "      <td>United States</td>\n",
       "      <td>...</td>\n",
       "      <td>2131632891</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>001-815-086-488-477; 007-447-979-011-061; 025-...</td>\n",
       "      <td>33</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>000-042-629-788-452</td>\n",
       "      <td>Assessment of a falling solid particle receive...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015</td>\n",
       "      <td>journal article</td>\n",
       "      <td>Solar Energy</td>\n",
       "      <td>0038092x</td>\n",
       "      <td>Elsevier BV</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>...</td>\n",
       "      <td>2067658121</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>002-994-869-371-657; 005-360-904-860-363; 008-...</td>\n",
       "      <td>62</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>green</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>000-062-946-383-801</td>\n",
       "      <td>Boron-Doped Silicon Diatom Frustules as a Phot...</td>\n",
       "      <td>2015-07-30</td>\n",
       "      <td>2015</td>\n",
       "      <td>journal article</td>\n",
       "      <td>ACS applied materials &amp; interfaces</td>\n",
       "      <td>19448252; 19448244</td>\n",
       "      <td>American Chemical Society (ACS)</td>\n",
       "      <td>United States</td>\n",
       "      <td>...</td>\n",
       "      <td>2410807451</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>004-049-858-705-806; 007-090-004-757-278; 011-...</td>\n",
       "      <td>26</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>000-063-768-273-718</td>\n",
       "      <td>Renewable energy management through microgrid ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015</td>\n",
       "      <td>journal article</td>\n",
       "      <td>Energy Reports</td>\n",
       "      <td>23524847</td>\n",
       "      <td>Elsevier BV</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>...</td>\n",
       "      <td>1193286937</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>004-077-674-704-399; 005-081-834-054-355; 011-...</td>\n",
       "      <td>79</td>\n",
       "      <td>True</td>\n",
       "      <td>CC BY, CC BY-NC-ND</td>\n",
       "      <td>gold</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>000-138-869-752-834</td>\n",
       "      <td>Thermal and electrical analysis of a linear pa...</td>\n",
       "      <td>2015-12-01</td>\n",
       "      <td>2015</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>3210564331</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0              Lens ID  \\\n",
       "0           0  000-008-268-360-004   \n",
       "1           2  000-042-629-788-452   \n",
       "2           4  000-062-946-383-801   \n",
       "3           5  000-063-768-273-718   \n",
       "4          11  000-138-869-752-834   \n",
       "\n",
       "                                               Title Date Published  \\\n",
       "0  Day-Ahead Solar Irradiance Forecasting in a Tr...     2015-07-27   \n",
       "1  Assessment of a falling solid particle receive...            NaN   \n",
       "2  Boron-Doped Silicon Diatom Frustules as a Phot...     2015-07-30   \n",
       "3  Renewable energy management through microgrid ...            NaN   \n",
       "4  Thermal and electrical analysis of a linear pa...     2015-12-01   \n",
       "\n",
       "   Publication Year Publication Type                         Source Title  \\\n",
       "0              2015  journal article  Journal of Solar Energy Engineering   \n",
       "1              2015  journal article                         Solar Energy   \n",
       "2              2015  journal article   ACS applied materials & interfaces   \n",
       "3              2015  journal article                       Energy Reports   \n",
       "4              2015              NaN                                  NaN   \n",
       "\n",
       "                ISSNs                        Publisher  Source Country  ...  \\\n",
       "0  01996231; 15288986               ASME International   United States  ...   \n",
       "1            0038092x                      Elsevier BV  United Kingdom  ...   \n",
       "2  19448252; 19448244  American Chemical Society (ACS)   United States  ...   \n",
       "3            23524847                      Elsevier BV  United Kingdom  ...   \n",
       "4                 NaN                              NaN             NaN  ...   \n",
       "\n",
       "  Microsoft Academic ID PMCID Citing Patents Count  \\\n",
       "0            2131632891   NaN                    0   \n",
       "1            2067658121   NaN                    0   \n",
       "2            2410807451   NaN                    0   \n",
       "3            1193286937   NaN                    0   \n",
       "4            3210564331   NaN                    0   \n",
       "\n",
       "                                          References Citing Works Count  \\\n",
       "0  001-815-086-488-477; 007-447-979-011-061; 025-...                 33   \n",
       "1  002-994-869-371-657; 005-360-904-860-363; 008-...                 62   \n",
       "2  004-049-858-705-806; 007-090-004-757-278; 011-...                 26   \n",
       "3  004-077-674-704-399; 005-081-834-054-355; 011-...                 79   \n",
       "4                                                NaN                  0   \n",
       "\n",
       "  Is Open Access Open Access License Open Access Colour title_languages  \\\n",
       "0          False                 NaN                NaN              en   \n",
       "1           True                 NaN              green              en   \n",
       "2          False                 NaN                NaN              en   \n",
       "3           True  CC BY, CC BY-NC-ND               gold              en   \n",
       "4          False                 NaN                NaN              en   \n",
       "\n",
       "  abstract_languages  \n",
       "0                 en  \n",
       "1                 en  \n",
       "2                 en  \n",
       "3                 en  \n",
       "4                 en  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv(\"cleaned_data/cleaned_data.csv\",sep=';')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(261492, 18843)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords=stopwords.words('english')\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=stopwords, min_df=5, max_df=0.7)\n",
    "tfidf_vectors = tfidf_vectorizer.fit_transform(data['Title'])\n",
    "tfidf_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class:  ndarray\n",
      "shape:  (261492, 50)\n",
      "strides:  (400, 8)\n",
      "itemsize:  8\n",
      "aligned:  True\n",
      "contiguous:  True\n",
      "fortran:  False\n",
      "data pointer: 0x29b6d331040\n",
      "byteorder:  little\n",
      "byteswap:  False\n",
      "type: float64\n"
     ]
    }
   ],
   "source": [
    "nmf_text_model = NMF(n_components=50, random_state=random_state) # n_components: number of topics\n",
    "W_text_matrix = nmf_text_model.fit_transform(tfidf_vectors)\n",
    "H_text_matrix = nmf_text_model.components_\n",
    "np.info(W_text_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class:  ndarray\n",
      "shape:  (50, 18843)\n",
      "strides:  (8, 400)\n",
      "itemsize:  8\n",
      "aligned:  True\n",
      "contiguous:  False\n",
      "fortran:  True\n",
      "data pointer: 0x29b0ed9b040\n",
      "byteorder:  little\n",
      "byteswap:  False\n",
      "type: float64\n"
     ]
    }
   ],
   "source": [
    "np.info(H_text_matrix) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic 00\n",
      "  system (34.67)\n",
      "  integrated (2.39)\n",
      "  tracking (1.94)\n",
      "  monitoring (1.79)\n",
      "  heating (1.64)\n",
      "\n",
      "Topic 01\n",
      "  perovskite (14.46)\n",
      "  cells (9.63)\n",
      "  stability (1.81)\n",
      "  stable (1.76)\n",
      "  inverted (1.29)\n",
      "\n",
      "Topic 02\n",
      "  sub (35.23)\n",
      "  co (2.06)\n",
      "  tio (1.49)\n",
      "  cu (1.07)\n",
      "  se (0.96)\n",
      "\n",
      "Topic 03\n",
      "  energy (17.77)\n",
      "  renewable (8.40)\n",
      "  sources (2.29)\n",
      "  harvesting (1.33)\n",
      "  resources (0.94)\n",
      "\n",
      "Topic 04\n",
      "  heat (11.86)\n",
      "  transfer (4.41)\n",
      "  pump (2.38)\n",
      "  air (1.97)\n",
      "  experimental (1.66)\n",
      "\n",
      "Topic 05\n",
      "  power (26.96)\n",
      "  plant (4.84)\n",
      "  plants (3.10)\n",
      "  maximum (2.73)\n",
      "  point (2.42)\n",
      "\n",
      "Topic 06\n",
      "  based (28.61)\n",
      "  iot (1.21)\n",
      "  method (1.02)\n",
      "  fuzzy (0.98)\n",
      "  algorithm (0.97)\n",
      "\n",
      "Topic 07\n",
      "  photovoltaic (29.68)\n",
      "  modules (2.49)\n",
      "  module (1.86)\n",
      "  panels (1.61)\n",
      "  maximum (1.10)\n",
      "\n",
      "Topic 08\n",
      "  storage (26.52)\n",
      "  energy (5.50)\n",
      "  battery (5.45)\n",
      "  conversion (1.36)\n",
      "  optimal (1.35)\n",
      "\n",
      "Topic 09\n",
      "  efficient (18.11)\n",
      "  highly (6.73)\n",
      "  stable (4.77)\n",
      "  polymer (1.04)\n",
      "  engineering (0.78)\n",
      "\n",
      "Topic 10\n",
      "  grid (23.82)\n",
      "  connected (11.70)\n",
      "  inverter (2.73)\n",
      "  micro (2.30)\n",
      "  control (2.21)\n",
      "\n",
      "Topic 11\n",
      "  using (23.86)\n",
      "  method (1.39)\n",
      "  algorithm (1.17)\n",
      "  technique (0.91)\n",
      "  fuzzy (0.87)\n",
      "\n",
      "Topic 12\n",
      "  study (17.90)\n",
      "  case (9.42)\n",
      "  experimental (2.25)\n",
      "  comparative (2.04)\n",
      "  feasibility (1.26)\n",
      "\n",
      "Topic 13\n",
      "  silicon (12.37)\n",
      "  cells (3.31)\n",
      "  heterojunction (2.86)\n",
      "  crystalline (2.58)\n",
      "  tandem (1.55)\n",
      "\n",
      "Topic 14\n",
      "  high (18.60)\n",
      "  temperature (3.89)\n",
      "  voltage (1.24)\n",
      "  penetration (1.19)\n",
      "  resolution (1.13)\n",
      "\n",
      "Topic 15\n",
      "  analysis (27.84)\n",
      "  comparative (2.35)\n",
      "  exergy (1.49)\n",
      "  different (0.96)\n",
      "  sensitivity (0.73)\n",
      "\n",
      "Topic 16\n",
      "  design (27.77)\n",
      "  implementation (2.62)\n",
      "  building (1.56)\n",
      "  simulation (1.39)\n",
      "  powered (1.12)\n",
      "\n",
      "Topic 17\n",
      "  hybrid (40.42)\n",
      "  inorganic (1.79)\n",
      "  battery (1.62)\n",
      "  diesel (1.46)\n",
      "  optimal (1.38)\n",
      "\n",
      "Topic 18\n",
      "  performance (24.29)\n",
      "  evaluation (3.80)\n",
      "  effect (2.44)\n",
      "  collector (1.31)\n",
      "  experimental (1.28)\n",
      "\n",
      "Topic 19\n",
      "  dye (12.42)\n",
      "  sensitized (11.43)\n",
      "  cells (5.34)\n",
      "  tio2 (2.07)\n",
      "  counter (1.48)\n",
      "\n",
      "Topic 20\n",
      "  materials (23.55)\n",
      "  conversion (2.36)\n",
      "  recent (2.02)\n",
      "  hole (2.01)\n",
      "  transporting (1.67)\n",
      "\n",
      "Topic 21\n",
      "  water (16.21)\n",
      "  splitting (6.48)\n",
      "  photoelectrochemical (2.54)\n",
      "  oxidation (1.42)\n",
      "  pumping (1.27)\n",
      "\n",
      "Topic 22\n",
      "  pv (24.06)\n",
      "  mppt (2.27)\n",
      "  battery (1.99)\n",
      "  module (1.56)\n",
      "  modules (1.42)\n",
      "\n",
      "Topic 23\n",
      "  organic (10.86)\n",
      "  cells (4.39)\n",
      "  fullerene (2.37)\n",
      "  polymer (2.27)\n",
      "  non (2.27)\n",
      "\n",
      "Topic 24\n",
      "  systems (33.77)\n",
      "  integrated (1.73)\n",
      "  heating (1.47)\n",
      "  distribution (1.33)\n",
      "  control (1.02)\n",
      "\n",
      "Topic 25\n",
      "  dc (13.77)\n",
      "  control (7.39)\n",
      "  converter (6.55)\n",
      "  microgrid (3.22)\n",
      "  voltage (2.48)\n",
      "\n",
      "Topic 26\n",
      "  thermal (23.25)\n",
      "  collector (2.79)\n",
      "  experimental (1.39)\n",
      "  collectors (1.21)\n",
      "  parabolic (1.19)\n",
      "\n",
      "Topic 27\n",
      "  network (4.20)\n",
      "  radiation (3.68)\n",
      "  neural (3.47)\n",
      "  artificial (2.46)\n",
      "  networks (2.18)\n",
      "\n",
      "Topic 28\n",
      "  thin (11.27)\n",
      "  film (7.02)\n",
      "  films (6.73)\n",
      "  deposition (1.55)\n",
      "  cu (1.12)\n",
      "\n",
      "Topic 29\n",
      "  generation (27.84)\n",
      "  electricity (5.14)\n",
      "  distributed (4.12)\n",
      "  steam (2.28)\n",
      "  next (1.59)\n",
      "\n",
      "Topic 30\n",
      "  hydrogen (12.89)\n",
      "  production (12.24)\n",
      "  evolution (2.33)\n",
      "  green (2.26)\n",
      "  photocatalytic (2.04)\n",
      "\n",
      "Topic 31\n",
      "  electric (16.01)\n",
      "  vehicle (9.93)\n",
      "  charging (9.60)\n",
      "  vehicles (5.13)\n",
      "  station (4.29)\n",
      "\n",
      "Topic 32\n",
      "  change (9.95)\n",
      "  phase (8.28)\n",
      "  climate (7.28)\n",
      "  material (4.05)\n",
      "  single (1.21)\n",
      "\n",
      "Topic 33\n",
      "  review (24.39)\n",
      "  comprehensive (3.52)\n",
      "  recent (2.15)\n",
      "  technologies (1.94)\n",
      "  techniques (1.93)\n",
      "\n",
      "Topic 34\n",
      "  metal (7.19)\n",
      "  halide (6.73)\n",
      "  perovskites (5.41)\n",
      "  lead (3.85)\n",
      "  free (2.55)\n",
      "\n",
      "Topic 35\n",
      "  solar (25.84)\n",
      "  cells (2.39)\n",
      "  powered (1.69)\n",
      "  panel (1.10)\n",
      "  energy (0.83)\n",
      "\n",
      "Topic 36\n",
      "  development (14.85)\n",
      "  sustainable (8.29)\n",
      "  green (1.73)\n",
      "  technology (1.66)\n",
      "  future (1.31)\n",
      "\n",
      "Topic 37\n",
      "  learning (16.57)\n",
      "  machine (11.89)\n",
      "  deep (5.39)\n",
      "  forecasting (4.93)\n",
      "  prediction (3.34)\n",
      "\n",
      "Topic 38\n",
      "  light (7.08)\n",
      "  photocatalytic (3.12)\n",
      "  visible (3.06)\n",
      "  enhanced (2.34)\n",
      "  driven (1.81)\n",
      "\n",
      "Topic 39\n",
      "  applications (25.09)\n",
      "  synthesis (3.19)\n",
      "  characterization (1.20)\n",
      "  graphene (1.16)\n",
      "  recent (1.09)\n",
      "\n",
      "Topic 40\n",
      "  economic (10.20)\n",
      "  assessment (6.53)\n",
      "  techno (4.95)\n",
      "  environmental (3.85)\n",
      "  cycle (2.24)\n",
      "\n",
      "Topic 41\n",
      "  carbon (12.00)\n",
      "  low (6.45)\n",
      "  cost (2.55)\n",
      "  dioxide (2.23)\n",
      "  nitride (1.36)\n",
      "\n",
      "Topic 42\n",
      "  optimization (20.64)\n",
      "  multi (5.63)\n",
      "  algorithm (4.32)\n",
      "  objective (2.80)\n",
      "  swarm (2.07)\n",
      "\n",
      "Topic 43\n",
      "  wind (20.46)\n",
      "  turbine (2.15)\n",
      "  optimal (1.02)\n",
      "  speed (1.02)\n",
      "  scale (0.85)\n",
      "\n",
      "Topic 44\n",
      "  model (15.21)\n",
      "  predictive (1.73)\n",
      "  control (1.39)\n",
      "  new (1.31)\n",
      "  simulation (1.05)\n",
      "\n",
      "Topic 45\n",
      "  properties (8.21)\n",
      "  optical (5.46)\n",
      "  effect (2.69)\n",
      "  structural (2.26)\n",
      "  electronic (2.07)\n",
      "\n",
      "Topic 46\n",
      "  efficiency (24.47)\n",
      "  conversion (4.11)\n",
      "  quantum (1.94)\n",
      "  stability (1.68)\n",
      "  improving (1.62)\n",
      "\n",
      "Topic 47\n",
      "  management (14.63)\n",
      "  smart (9.92)\n",
      "  microgrid (3.63)\n",
      "  demand (2.34)\n",
      "  optimal (2.12)\n",
      "\n",
      "Topic 48\n",
      "  cell (23.05)\n",
      "  fuel (3.45)\n",
      "  simulation (3.35)\n",
      "  solar (2.53)\n",
      "  modeling (1.58)\n",
      "\n",
      "Topic 49\n",
      "  transport (8.90)\n",
      "  layer (8.78)\n",
      "  electron (6.54)\n",
      "  hole (4.83)\n",
      "  charge (2.32)\n"
     ]
    }
   ],
   "source": [
    "def display_topics(model, features, no_top_words=5):\n",
    "    for topic, words in enumerate(model.components_):\n",
    "        total = words.sum()\n",
    "        largest = words.argsort()[::-1] # invert sort order\n",
    "        print(\"\\nTopic %02d\" % topic)\n",
    "        for i in range(0, no_top_words):\n",
    "            print(\"  %s (%2.2f)\" % (features[largest[i]], abs(words[largest[i]]*100.0/total)))\n",
    "            \n",
    "display_topics(nmf_text_model, tfidf_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.87095357 2.62678808 2.23239244 5.36652753 1.07878288 3.16514028\n",
      " 1.47759932 2.5604315  1.69553754 1.24198854 1.36224818 1.90235875\n",
      " 2.59216159 1.72222155 1.29801461 1.69612975 1.07877677 1.7063228\n",
      " 1.70342245 1.3692708  0.7727497  1.30684719 1.53211043 2.09078475\n",
      " 1.09640296 1.23488685 2.57981434 1.47800615 1.29636349 1.61959611\n",
      " 1.14839945 1.73870892 1.62590339 1.51848556 1.66359807 5.35555311\n",
      " 2.21201659 1.17579536 3.12923051 1.65554742 2.93536575 2.67188662\n",
      " 2.17743481 1.85612594 2.37852597 3.53116011 2.31022786 2.14771681\n",
      " 2.48885462 2.52483222]\n"
     ]
    }
   ],
   "source": [
    "# Find out how “big” the topics are, i.e., how many documents could be assigned mainly to each topic. \n",
    "# This can be calculated using the document-topic matrix and \n",
    "# summing the individual topic contributions over all documents.\n",
    "# Normalizing them with the total sum and multiplying by 100 gives a percentage value:\n",
    "\n",
    "print(W_text_matrix.sum(axis=0)/W_text_matrix.sum()*100.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "n_top_words = 5\n",
    "\n",
    "# Dokument-Topic-Zuordnung anzeigen\n",
    "# for doc_idx, topic_dist in enumerate(W_text_matrix):\n",
    "#     print(f\"Dokument {doc_idx}:\")\n",
    "#     print(\" \".join([f\"Topic {i}: {topic_dist[i]:.2f}\" for i in np.argsort(topic_dist)[::-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic Kohärenz: 0.73\n"
     ]
    }
   ],
   "source": [
    "# Gensim Dictionary und Corpus erstellen\n",
    "texts = [doc.split() for doc in data.Title.to_list()]\n",
    "dictionary = Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# Topics in Gensim-Format umwandeln\n",
    "topics = [[feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]] for topic in H_text_matrix]\n",
    "\n",
    "# Kohärenzmodell erstellen\n",
    "coherence_model = CoherenceModel(topics=topics, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "coherence_score = coherence_model.get_coherence()\n",
    "print(f\"Topic Kohärenz: {coherence_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = tfidf_vectors\n",
    "# # Elbow-Methode\n",
    "# errors = []\n",
    "# for n_topics in range(1, 51):\n",
    "#     nmf = NMF(n_components=n_topics, random_state=random_state)\n",
    "#     W = nmf.fit_transform(X)\n",
    "#     H = nmf.components_\n",
    "#     reconstruction_error = nmf.reconstruction_err_\n",
    "#     errors.append(reconstruction_error)\n",
    "\n",
    "# plt.plot(range(1, 51), errors, marker='o')\n",
    "# plt.xlabel('Anzahl der Topics')\n",
    "# plt.ylabel('Rekonstruktionsfehler')\n",
    "# plt.title('Elbow-Methode zur Bestimmung der optimalen Anzahl der Topics')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'words'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m stopwords2\u001b[38;5;241m=\u001b[39mstopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m stopwords3\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(STOPWORDS)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'words'"
     ]
    }
   ],
   "source": [
    "# stopwords2=stopwords.words('english')\n",
    "# stopwords3=list(STOPWORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No sqlite db found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-13 14:02:09,944] A new study created in RDB with name: topicmodeling_nmf_titles\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05c6f832d3d74974a577108ce3857161",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lekle\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-03-13 14:06:00,917] Trial 0 finished with value: 0.6397847363938021 and parameters: {'max_df': 0.650455351094064, 'min_df': 77, 'n_components': 100}. Best is trial 0 with value: 0.6397847363938021.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lekle\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-03-13 14:07:14,484] Trial 1 finished with value: 0.7192827811680982 and parameters: {'max_df': 0.766598237957762, 'min_df': 65, 'n_components': 25}. Best is trial 1 with value: 0.7192827811680982.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lekle\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-03-13 14:08:40,444] Trial 2 finished with value: 0.7325877899803318 and parameters: {'max_df': 0.5979761112443638, 'min_df': 93, 'n_components': 25}. Best is trial 2 with value: 0.7325877899803318.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lekle\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-03-13 14:11:27,259] Trial 3 finished with value: 0.6586225875876702 and parameters: {'max_df': 0.6182213394056657, 'min_df': 99, 'n_components': 100}. Best is trial 2 with value: 0.7325877899803318.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lekle\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-03-13 14:12:27,924] Trial 4 finished with value: 0.7253924349232928 and parameters: {'max_df': 0.7245133175751064, 'min_df': 76, 'n_components': 25}. Best is trial 2 with value: 0.7325877899803318.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lekle\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['c:/Users/lekle/Projects_Code/HA_TextAnalytics/models/topicmodeling_nmf_titles/topicmodeling_nmf_titles.pkl']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "studyname='topicmodeling_nmf_titles'\n",
    "thismodelpath=models_path.joinpath(f'{studyname}')\n",
    "try: \n",
    "    thismodelpath.mkdir(exist_ok=False)\n",
    "except FileExistsError:\n",
    "    print('Directory already exists')\n",
    "\n",
    "\n",
    "def inst_tfidf(trial:Trial)->TfidfVectorizer:\n",
    "    params={\n",
    "        'stop_words':'english',\n",
    "        'tokenizer':tokenize,\n",
    "        # 'max_features':trial.suggest_categorical('max_features',[None,1000,5000,10000])\n",
    "        'max_df':trial.suggest_float('max_df',0.5,1),\n",
    "        'min_df':trial.suggest_int('min_df',50,100)\n",
    "    }\n",
    "    return TfidfVectorizer(**params)\n",
    "\n",
    "def inst_nmf(trial:Trial)->NMF:\n",
    "    params={\n",
    "        'random_state':random_state,\n",
    "        'n_components':trial.suggest_categorical('n_components',[25,50,100])\n",
    "    }\n",
    "    return NMF(**params)\n",
    "def inst_pipe_nmf(trial:Trial)->Pipeline:\n",
    "    pipeline=Pipeline([\n",
    "        ('tfidf',inst_tfidf(trial)),\n",
    "        ('nmf',inst_nmf(trial))\n",
    "    ])\n",
    "    return pipeline\n",
    "def objective(trial:Trial,x:pd.DataFrame)->float:\n",
    "    model=inst_pipe_nmf(trial)\n",
    "    # print(type(model))\n",
    "    fitmodel=model.fit(x)\n",
    "    W_text_matrix = fitmodel.transform(x)\n",
    "    H_text_matrix = fitmodel.named_steps['nmf'].components_\n",
    "    feature_names = fitmodel.named_steps['tfidf'].get_feature_names_out()\n",
    "    n_top_words = 5\n",
    "    texts=[doc.split() for doc in x.to_list()]\n",
    "    dictionary = Dictionary(texts)\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "    topics = [[feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]] for topic in H_text_matrix]\n",
    "\n",
    "    coherence_model = CoherenceModel(topics=topics, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_score = coherence_model.get_coherence()\n",
    "    return coherence_score\n",
    "\n",
    "\n",
    "storage=thismodelpath.joinpath(f\"{studyname}.db\")\n",
    "if storage.exists():\n",
    "    storage.unlink()\n",
    "else:\n",
    "    print(\"No sqlite db found\")\n",
    "\n",
    "study=create_study(study_name=studyname,direction='maximize',storage=f'sqlite:///{storage.as_posix()}',load_if_exists=False) #TPESampler used as default, no pruning\n",
    "study.optimize(lambda trial: objective(trial,data['Title']),n_trials=5,n_jobs=1,show_progress_bar=True)\n",
    "joblib.dump(study,thismodelpath.joinpath(f'study_{study.study_name}.pkl').as_posix())\n",
    "\n",
    "model=inst_pipe_nmf(study.best_trial)\n",
    "fitpipe=model.fit(data['Title'])\n",
    "joblib.dump(fitpipe,thismodelpath.joinpath(f'{studyname}.pkl').as_posix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-13 14:13:15,379] A new study created in RDB with name: topicmodeling_nmf_abstracts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No sqlite db found\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5cbaad5c76a4f9aace07b1051a776d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lekle\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\lekle\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1770: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-03-13 14:20:16,283] Trial 0 finished with value: 0.6354691778653848 and parameters: {'max_df': 0.6891507577945581, 'min_df': 95, 'n_components': 25}. Best is trial 0 with value: 0.6354691778653848.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lekle\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-03-13 14:26:01,819] Trial 1 finished with value: 0.6345680231546599 and parameters: {'max_df': 0.6734960742597685, 'min_df': 50, 'n_components': 25}. Best is trial 0 with value: 0.6354691778653848.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lekle\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\lekle\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1770: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-03-13 14:35:40,515] Trial 2 finished with value: 0.6891083840554937 and parameters: {'max_df': 0.7187419836776672, 'min_df': 69, 'n_components': 50}. Best is trial 2 with value: 0.6891083840554937.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lekle\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-03-13 14:43:19,241] Trial 3 finished with value: 0.6730490971147014 and parameters: {'max_df': 0.7910111282722723, 'min_df': 92, 'n_components': 50}. Best is trial 2 with value: 0.6891083840554937.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lekle\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-03-13 14:49:36,653] Trial 4 finished with value: 0.6451135532928111 and parameters: {'max_df': 0.5613341821704063, 'min_df': 88, 'n_components': 25}. Best is trial 2 with value: 0.6891083840554937.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lekle\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\lekle\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1770: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['c:/Users/lekle/Projects_Code/HA_TextAnalytics/models/topicmodeling_nmf_abstracts/topicmodeling_nmf_abstracts.pkl']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "studyname='topicmodeling_nmf_abstracts'\n",
    "thismodelpath=models_path.joinpath(f'{studyname}')\n",
    "try: \n",
    "    thismodelpath.mkdir(exist_ok=False)\n",
    "except FileExistsError:\n",
    "    print('Directory already exists')\n",
    "\n",
    "\n",
    "def inst_tfidf(trial:Trial)->TfidfVectorizer:\n",
    "    params={\n",
    "        'stop_words': 'english',#trial.suggest_categorical('stop_words',['english',stopwords.words('english'),list(STOPWORDS)]),\n",
    "        'tokenizer':tokenize,\n",
    "        # 'max_features':trial.suggest_categorical('max_features',[None,1000,5000,10000])\n",
    "        'max_df':trial.suggest_float('max_df',0.5,1),\n",
    "        'min_df':trial.suggest_int('min_df',50,100)\n",
    "    }\n",
    "    return TfidfVectorizer(**params)\n",
    "\n",
    "def inst_nmf(trial:Trial)->NMF:\n",
    "    params={\n",
    "        'random_state':random_state,\n",
    "        'n_components':trial.suggest_categorical('n_components',[25,50,100])\n",
    "    }\n",
    "    return NMF(**params)\n",
    "def inst_pipe_nmf(trial:Trial)->Pipeline:\n",
    "    pipeline=Pipeline([\n",
    "        ('tfidf',inst_tfidf(trial)),\n",
    "        ('nmf',inst_nmf(trial))\n",
    "    ])\n",
    "    return pipeline\n",
    "\n",
    "def objective(trial:Trial,x:pd.DataFrame)->float:\n",
    "    model=inst_pipe_nmf(trial)\n",
    "    fitmodel=model.fit(x)\n",
    "    W_text_matrix = fitmodel.transform(x)\n",
    "    H_text_matrix = fitmodel.named_steps['nmf'].components_\n",
    "    feature_names = fitmodel.named_steps['tfidf'].get_feature_names_out()\n",
    "    n_top_words = 5\n",
    "    texts=[doc.split() for doc in x.to_list()]\n",
    "    dictionary = Dictionary(texts)\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "    topics = [[feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]] for topic in H_text_matrix]\n",
    "\n",
    "    coherence_model = CoherenceModel(topics=topics, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_score = coherence_model.get_coherence()\n",
    "    return coherence_score\n",
    "\n",
    "\n",
    "storage=thismodelpath.joinpath(f\"{studyname}.db\")\n",
    "if storage.exists():\n",
    "    storage.unlink()\n",
    "else:\n",
    "    print(\"No sqlite db found\")\n",
    "\n",
    "study=create_study(study_name=studyname,direction='maximize',storage=f'sqlite:///{storage.as_posix()}',load_if_exists=False) #TPESampler used as default, no pruning\n",
    "study.optimize(lambda trial: objective(trial,data['Abstract']),n_trials=5,n_jobs=1,show_progress_bar=True)\n",
    "joblib.dump(study,thismodelpath.joinpath(f'study_{study.study_name}.pkl').as_posix())\n",
    "\n",
    "model=inst_pipe_nmf(study.best_trial)\n",
    "fitpipe=model.fit(data['Abstract'])\n",
    "joblib.dump(fitpipe,thismodelpath.joinpath(f'{studyname}.pkl').as_posix())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-13 14:55:27,453] A new study created in RDB with name: topicmodeling_lsa_titles\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No sqlite db found\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2321124533a04ae8abfdf3fb651fc21c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lekle\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-03-13 14:56:14,566] Trial 0 finished with value: 0.5047244113156633 and parameters: {'max_df': 0.6783536855114765, 'min_df': 96, 'n_components': 100}. Best is trial 0 with value: 0.5047244113156633.\n",
      "[I 2025-03-13 14:56:58,195] Trial 1 finished with value: 0.5515840099930707 and parameters: {'max_df': 0.6463086974169041, 'min_df': 62, 'n_components': 50}. Best is trial 1 with value: 0.5515840099930707.\n",
      "[I 2025-03-13 14:57:42,345] Trial 2 finished with value: 0.5449357231243231 and parameters: {'max_df': 0.5387949524522034, 'min_df': 52, 'n_components': 50}. Best is trial 1 with value: 0.5515840099930707.\n",
      "[I 2025-03-13 14:58:26,605] Trial 3 finished with value: 0.5480249769383991 and parameters: {'max_df': 0.8508103793346755, 'min_df': 63, 'n_components': 50}. Best is trial 1 with value: 0.5515840099930707.\n",
      "[I 2025-03-13 14:59:09,963] Trial 4 finished with value: 0.6100544446426033 and parameters: {'max_df': 0.934718896433256, 'min_df': 96, 'n_components': 25}. Best is trial 4 with value: 0.6100544446426033.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-13 14:59:16,577] A new study created in RDB with name: topicmodeling_lsa_abstracts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No sqlite db found\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0d3023b8f814c47baca1560e627a5f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-03-13 15:03:55,193] Trial 0 finished with value: 0.4119163491540576 and parameters: {'max_df': 0.8747774290530379, 'min_df': 74, 'n_components': 100}. Best is trial 0 with value: 0.4119163491540576.\n",
      "[I 2025-03-13 15:08:32,764] Trial 1 finished with value: 0.4273542777048634 and parameters: {'max_df': 0.6940911864504777, 'min_df': 74, 'n_components': 100}. Best is trial 1 with value: 0.4273542777048634.\n",
      "[I 2025-03-13 15:13:03,952] Trial 2 finished with value: 0.4214871589124367 and parameters: {'max_df': 0.9197644916836246, 'min_df': 54, 'n_components': 50}. Best is trial 1 with value: 0.4273542777048634.\n",
      "[I 2025-03-13 15:17:37,991] Trial 3 finished with value: 0.43069358848556516 and parameters: {'max_df': 0.8348404247092509, 'min_df': 92, 'n_components': 50}. Best is trial 3 with value: 0.43069358848556516.\n",
      "[I 2025-03-13 15:22:04,816] Trial 4 finished with value: 0.44802853094494954 and parameters: {'max_df': 0.9992828269461216, 'min_df': 76, 'n_components': 25}. Best is trial 4 with value: 0.44802853094494954.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['c:/Users/lekle/Projects_Code/HA_TextAnalytics/models/topicmodeling_lsa_abstracts/topicmodeling_lsa_abstracts.pkl']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "studyname='topicmodeling_lsa_titles'\n",
    "thismodelpath=models_path.joinpath(f'{studyname}')\n",
    "try: \n",
    "    thismodelpath.mkdir(exist_ok=False)\n",
    "except FileExistsError:\n",
    "    print('Directory already exists')\n",
    "\n",
    "\n",
    "def inst_tfidf(trial:Trial)->TfidfVectorizer:\n",
    "    params={\n",
    "        'stop_words':'english',\n",
    "        'tokenizer':tokenize,\n",
    "        # 'max_features':trial.suggest_categorical('max_features',[None,1000,5000,10000])\n",
    "        'max_df':trial.suggest_float('max_df',0.5,1),\n",
    "        'min_df':trial.suggest_int('min_df',50,100)\n",
    "    }\n",
    "    return TfidfVectorizer(**params)\n",
    "\n",
    "def inst_lsa(trial:Trial)->TruncatedSVD:\n",
    "    params={\n",
    "        'random_state':random_state,\n",
    "        'n_components':trial.suggest_categorical('n_components',[25,50,100])\n",
    "    }\n",
    "    return TruncatedSVD(**params)\n",
    "def inst_pipe_lsa(trial:Trial)->Pipeline:\n",
    "    pipeline=Pipeline([\n",
    "        ('tfidf',inst_tfidf(trial)),\n",
    "        ('lsa',inst_lsa(trial))\n",
    "    ])\n",
    "    return pipeline\n",
    "def objective(trial:Trial,x:pd.DataFrame)->float:\n",
    "    model=inst_pipe_lsa(trial)\n",
    "    # print(type(model))\n",
    "    fitmodel=model.fit(x)\n",
    "    W_text_matrix = fitmodel.transform(x)\n",
    "    H_text_matrix = fitmodel.named_steps['lsa'].components_\n",
    "    feature_names = fitmodel.named_steps['tfidf'].get_feature_names_out()\n",
    "    n_top_words = 5\n",
    "    texts=[doc.split() for doc in x.to_list()]\n",
    "    dictionary = Dictionary(texts)\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "    topics = [[feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]] for topic in H_text_matrix]\n",
    "\n",
    "    coherence_model = CoherenceModel(topics=topics, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_score = coherence_model.get_coherence()\n",
    "    return coherence_score\n",
    "\n",
    "\n",
    "storage=thismodelpath.joinpath(f\"{studyname}.db\")\n",
    "if storage.exists():\n",
    "    storage.unlink()\n",
    "else:\n",
    "    print(\"No sqlite db found\")\n",
    "\n",
    "study=create_study(study_name=studyname,direction='maximize',storage=f'sqlite:///{storage.as_posix()}',load_if_exists=False) #TPESampler used as default, no pruning\n",
    "study.optimize(lambda trial: objective(trial,data['Title']),n_trials=5,n_jobs=1,show_progress_bar=True)\n",
    "joblib.dump(study,thismodelpath.joinpath(f'study_{study.study_name}.pkl').as_posix())\n",
    "\n",
    "model=inst_pipe_lsa(study.best_trial)\n",
    "fitpipe=model.fit(data['Title'])\n",
    "joblib.dump(fitpipe,thismodelpath.joinpath(f'{studyname}.pkl').as_posix())\n",
    "\n",
    "\n",
    "studyname='topicmodeling_lsa_abstracts'\n",
    "thismodelpath=models_path.joinpath(f'{studyname}')\n",
    "try: \n",
    "    thismodelpath.mkdir(exist_ok=False)\n",
    "except FileExistsError:\n",
    "    print('Directory already exists')\n",
    "\n",
    "\n",
    "def inst_tfidf(trial:Trial)->TfidfVectorizer:\n",
    "    params={\n",
    "        'stop_words': 'english',#trial.suggest_categorical('stop_words',['english',stopwords.words('english'),list(STOPWORDS)]),\n",
    "        'tokenizer':tokenize,\n",
    "        # 'max_features':trial.suggest_categorical('max_features',[None,1000,5000,10000])\n",
    "        'max_df':trial.suggest_float('max_df',0.5,1),\n",
    "        'min_df':trial.suggest_int('min_df',50,100)\n",
    "    }\n",
    "    return TfidfVectorizer(**params)\n",
    "\n",
    "def inst_lsa(trial:Trial)->TruncatedSVD:\n",
    "    params={\n",
    "        'random_state':random_state,\n",
    "        'n_components':trial.suggest_categorical('n_components',[25,50,100])\n",
    "    }\n",
    "    return TruncatedSVD(**params)\n",
    "def inst_pipe_lsa(trial:Trial)->Pipeline:\n",
    "    pipeline=Pipeline([\n",
    "        ('tfidf',inst_tfidf(trial)),\n",
    "        ('lsa',inst_lsa(trial))\n",
    "    ])\n",
    "    return pipeline\n",
    "\n",
    "def objective(trial:Trial,x:pd.DataFrame)->float:\n",
    "    model=inst_pipe_lsa(trial)\n",
    "    fitmodel=model.fit(x)\n",
    "    W_text_matrix = fitmodel.transform(x)\n",
    "    H_text_matrix = fitmodel.named_steps['lsa'].components_\n",
    "    feature_names = fitmodel.named_steps['tfidf'].get_feature_names_out()\n",
    "    n_top_words = 5\n",
    "    texts=[doc.split() for doc in x.to_list()]\n",
    "    dictionary = Dictionary(texts)\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "    topics = [[feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]] for topic in H_text_matrix]\n",
    "\n",
    "    coherence_model = CoherenceModel(topics=topics, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_score = coherence_model.get_coherence()\n",
    "    return coherence_score\n",
    "\n",
    "\n",
    "storage=thismodelpath.joinpath(f\"{studyname}.db\")\n",
    "if storage.exists():\n",
    "    storage.unlink()\n",
    "else:\n",
    "    print(\"No sqlite db found\")\n",
    "\n",
    "study=create_study(study_name=studyname,direction='maximize',storage=f'sqlite:///{storage.as_posix()}',load_if_exists=False) #TPESampler used as default, no pruning\n",
    "study.optimize(lambda trial: objective(trial,data['Abstract']),n_trials=5,n_jobs=1,show_progress_bar=True)\n",
    "joblib.dump(study,thismodelpath.joinpath(f'study_{study.study_name}.pkl').as_posix())\n",
    "\n",
    "model=inst_pipe_lsa(study.best_trial)\n",
    "fitpipe=model.fit(data['Abstract'])\n",
    "joblib.dump(fitpipe,thismodelpath.joinpath(f'{studyname}.pkl').as_posix())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-13 15:23:09,106] A new study created in RDB with name: topicmodeling_lda_titles\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No sqlite db found\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c878e8aa64d94e3eaf76ad713ffaa3e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-03-13 15:27:38,347] Trial 0 finished with value: 0.5987675005606843 and parameters: {'max_df': 0.7851732899444197, 'min_df': 52, 'n_components': 50}. Best is trial 0 with value: 0.5987675005606843.\n",
      "[I 2025-03-13 15:32:24,967] Trial 1 finished with value: 0.4652117167236875 and parameters: {'max_df': 0.7404360734215389, 'min_df': 50, 'n_components': 100}. Best is trial 0 with value: 0.5987675005606843.\n",
      "[I 2025-03-13 15:36:50,202] Trial 2 finished with value: 0.609262706609892 and parameters: {'max_df': 0.6731187135180453, 'min_df': 99, 'n_components': 50}. Best is trial 2 with value: 0.609262706609892.\n",
      "[I 2025-03-13 15:41:35,635] Trial 3 finished with value: 0.48539497830996614 and parameters: {'max_df': 0.6335546893597586, 'min_df': 65, 'n_components': 100}. Best is trial 2 with value: 0.609262706609892.\n",
      "[I 2025-03-13 15:46:14,389] Trial 4 finished with value: 0.4750799382484194 and parameters: {'max_df': 0.8832113098747669, 'min_df': 100, 'n_components': 100}. Best is trial 2 with value: 0.609262706609892.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-13 15:49:43,206] A new study created in RDB with name: topicmodeling_lda_abstracts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No sqlite db found\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfc24abe305a4aee83c993f1dd697e14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-03-13 16:01:54,583] Trial 0 finished with value: 0.6322281547317193 and parameters: {'max_df': 0.7006981379853587, 'min_df': 79, 'n_components': 25}. Best is trial 0 with value: 0.6322281547317193.\n",
      "[I 2025-03-13 16:14:11,162] Trial 1 finished with value: 0.6361343498235509 and parameters: {'max_df': 0.6467333497441827, 'min_df': 80, 'n_components': 25}. Best is trial 1 with value: 0.6361343498235509.\n",
      "[I 2025-03-13 16:28:22,359] Trial 2 finished with value: 0.6340936609497568 and parameters: {'max_df': 0.5772748838644186, 'min_df': 65, 'n_components': 50}. Best is trial 1 with value: 0.6361343498235509.\n",
      "[I 2025-03-13 16:40:39,228] Trial 3 finished with value: 0.5684046594785218 and parameters: {'max_df': 0.765422897691731, 'min_df': 51, 'n_components': 25}. Best is trial 1 with value: 0.6361343498235509.\n",
      "[I 2025-03-13 16:52:18,397] Trial 4 finished with value: 0.6417774208657946 and parameters: {'max_df': 0.9926183102962507, 'min_df': 73, 'n_components': 10}. Best is trial 4 with value: 0.6417774208657946.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['c:/Users/lekle/Projects_Code/HA_TextAnalytics/models/topicmodeling_lda_abstracts/topicmodeling_lda_abstracts.pkl']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "studyname='topicmodeling_lda_titles'\n",
    "thismodelpath=models_path.joinpath(f'{studyname}')\n",
    "try: \n",
    "    thismodelpath.mkdir(exist_ok=False)\n",
    "except FileExistsError:\n",
    "    print('Directory already exists')\n",
    "\n",
    "\n",
    "def inst_tfidf(trial:Trial)->TfidfVectorizer:\n",
    "    params={\n",
    "        'stop_words':'english',\n",
    "        'tokenizer':tokenize,\n",
    "        # 'max_features':trial.suggest_categorical('max_features',[None,1000,5000,10000])\n",
    "        'max_df':trial.suggest_float('max_df',0.5,1),\n",
    "        'min_df':trial.suggest_int('min_df',50,100)\n",
    "    }\n",
    "    return TfidfVectorizer(**params)\n",
    "\n",
    "def inst_lda(trial:Trial)->LatentDirichletAllocation:\n",
    "    params={\n",
    "        'random_state':random_state,\n",
    "        'n_components':trial.suggest_categorical('n_components',[25,50,100])\n",
    "    }\n",
    "    return LatentDirichletAllocation(**params)\n",
    "def inst_pipe_lda(trial:Trial)->Pipeline:\n",
    "    pipeline=Pipeline([\n",
    "        ('tfidf',inst_tfidf(trial)),\n",
    "        ('lda',inst_lda(trial))\n",
    "    ])\n",
    "    return pipeline\n",
    "def objective(trial:Trial,x:pd.DataFrame)->float:\n",
    "    model=inst_pipe_lda(trial)\n",
    "    # print(type(model))\n",
    "    fitmodel=model.fit(x)\n",
    "    W_text_matrix = fitmodel.transform(x)\n",
    "    H_text_matrix = fitmodel.named_steps['lda'].components_\n",
    "    feature_names = fitmodel.named_steps['tfidf'].get_feature_names_out()\n",
    "    n_top_words = 5\n",
    "    texts=[doc.split() for doc in x.to_list()]\n",
    "    dictionary = Dictionary(texts)\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "    topics = [[feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]] for topic in H_text_matrix]\n",
    "\n",
    "    coherence_model = CoherenceModel(topics=topics, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_score = coherence_model.get_coherence()\n",
    "    return coherence_score\n",
    "\n",
    "\n",
    "storage=thismodelpath.joinpath(f\"{studyname}.db\")\n",
    "if storage.exists():\n",
    "    storage.unlink()\n",
    "else:\n",
    "    print(\"No sqlite db found\")\n",
    "\n",
    "study=create_study(study_name=studyname,direction='maximize',storage=f'sqlite:///{storage.as_posix()}',load_if_exists=False) #TPESampler used as default, no pruning\n",
    "study.optimize(lambda trial: objective(trial,data['Title']),n_trials=5,n_jobs=1,show_progress_bar=True)\n",
    "joblib.dump(study,thismodelpath.joinpath(f'study_{study.study_name}.pkl').as_posix())\n",
    "\n",
    "model=inst_pipe_lda(study.best_trial)\n",
    "fitpipe=model.fit(data['Title'])\n",
    "joblib.dump(fitpipe,thismodelpath.joinpath(f'{studyname}.pkl').as_posix())\n",
    "\n",
    "\n",
    "studyname='topicmodeling_lda_abstracts'\n",
    "thismodelpath=models_path.joinpath(f'{studyname}')\n",
    "try: \n",
    "    thismodelpath.mkdir(exist_ok=False)\n",
    "except FileExistsError:\n",
    "    print('Directory already exists')\n",
    "\n",
    "\n",
    "def inst_tfidf(trial:Trial)->TfidfVectorizer:\n",
    "    params={\n",
    "        'stop_words': 'english',#trial.suggest_categorical('stop_words',['english',stopwords.words('english'),list(STOPWORDS)]),\n",
    "        'tokenizer':tokenize,\n",
    "        # 'max_features':trial.suggest_categorical('max_features',[None,1000,5000,10000])\n",
    "        'max_df':trial.suggest_float('max_df',0.5,1),\n",
    "        'min_df':trial.suggest_int('min_df',50,100)\n",
    "    }\n",
    "    return TfidfVectorizer(**params)\n",
    "\n",
    "def inst_lda(trial:Trial)->LatentDirichletAllocation:\n",
    "    params={\n",
    "        'random_state':random_state,\n",
    "        'n_components':trial.suggest_categorical('n_components',[5,10,25,50]) #100 runs into errors\n",
    "    }\n",
    "    return LatentDirichletAllocation(**params)\n",
    "def inst_pipe_lda(trial:Trial)->Pipeline:\n",
    "    pipeline=Pipeline([\n",
    "        ('tfidf',inst_tfidf(trial)),\n",
    "        ('lda',inst_lda(trial))\n",
    "    ])\n",
    "    return pipeline\n",
    "\n",
    "def objective(trial:Trial,x:pd.DataFrame)->float:\n",
    "    model=inst_pipe_lda(trial)\n",
    "    fitmodel=model.fit(x)\n",
    "    W_text_matrix = fitmodel.transform(x)\n",
    "    H_text_matrix = fitmodel.named_steps['lda'].components_\n",
    "    feature_names = fitmodel.named_steps['tfidf'].get_feature_names_out()\n",
    "    n_top_words = 5\n",
    "    texts=[doc.split() for doc in x.to_list()]\n",
    "    dictionary = Dictionary(texts)\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "    topics = [[feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]] for topic in H_text_matrix]\n",
    "\n",
    "    coherence_model = CoherenceModel(topics=topics, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_score = coherence_model.get_coherence()\n",
    "    return coherence_score\n",
    "\n",
    "\n",
    "storage=thismodelpath.joinpath(f\"{studyname}.db\")\n",
    "if storage.exists():\n",
    "    storage.unlink()\n",
    "else:\n",
    "    print(\"No sqlite db found\")\n",
    "\n",
    "study=create_study(study_name=studyname,direction='maximize',storage=f'sqlite:///{storage.as_posix()}',load_if_exists=False) #TPESampler used as default, no pruning\n",
    "study.optimize(lambda trial: objective(trial,data['Abstract']),n_trials=5,n_jobs=1,show_progress_bar=True)\n",
    "joblib.dump(study,thismodelpath.joinpath(f'study_{study.study_name}.pkl').as_posix())\n",
    "\n",
    "model=inst_pipe_lda(study.best_trial)\n",
    "fitpipe=model.fit(data['Abstract'])\n",
    "joblib.dump(fitpipe,thismodelpath.joinpath(f'{studyname}.pkl').as_posix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
